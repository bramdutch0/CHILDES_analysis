'''
Arfiticial Data Experiment
This program extracts a list of constructions from a CHILDES
file and tests a set of learners on the data to see the order 
that constructions are acquired and the speed at which constructions
are acquired. This program tests both a uniform distribution over 
the constructions present and an observed distribution based on the
frequency in the CHILDES files.

To run this program use "python artificial_data_experiment.py [directory
where data is stored]". The directory should contain files with preparsed data of the form
*[speaker]: [Utterance]
%gra: [parse of sentence]
And should use "CHI" to indicate the child is speaking

A list of learners is set up in the main() function. This list can be altered.
Documentation on the parameters for different learners can be found in 
Learner.py

The output of this program is stored in "results/artificial_data/[directory where 
data is stored]
Output includes 
uniform/number_constructions- the average number of constructions every 10 time steps averaged
	over all of the experiments for a uniform distribution
uniform/order- the average order constructions were acquired over all the experiments
	for a uniform distribution
observed/number_constructions- the average number of constructions every 10 time steps averaged
	over all of the experiments for an observed distribution
observed/order- the average order constructions were acquired over all the experiments
	for an observed distribution

The order and number of constructions for each trial are also stored in seperate folders

'''

import Extract_data
import Helper
import Learner
import numpy as np
import pandas as pd 
import os
import operator
import sys

DATA_DIR = "Sachs"
NUM_TIME_STEPS = 100
TIMES_TO_RUN = 30
DIVISIONS = 10
UNIFORM_OUT_DIRECTORY = "results/theoretical_experiments/uniform"
OBSERVED_OUT_DIRECTORY = "results/theoretical_experiments/results_observed"

'''
Get results

	purpose:use the files generated by run_theoretical_experiments to 
			get the average number of constructions known by each 
			learner at each time step and the average order that 
			each learner learns the constructions
	inputs:	directory is the directory where the files are stored
			learners is the list of learner names (should be a list of
				strings)
			constructions is a list of constructions
	outputs:produces two files:
				1) directory/number_constructions
				2) directory/order
				which have similar layouts as the files created by 
				run_theoretical_experiments, but averaged over all of 
				the data
'''
def get_results(directory, learners, constructions):
	print("Getting results")

	#num_constructions is a dictionary mapping from names of learners to
	#a list of construction numbers at each time step
	num_constructions = {}
	#order is a dictionary mapping from names of learners to a dictionary
	#of construction oders
	order = {}
	for learner in learners:
		num_constructions[learner] = [0 for i in range(NUM_TIME_STEPS)]
		order[learner] = {}
		for const in constructions:
			order[learner][const] = 0

	#get the names of the corresponding directories
	number_const_dir = directory + "/number_constructions/"
	order_dir = directory + "/order/"

	
	##########################################
	#			Num_constructions
	##########################################
	number_list = os.listdir(number_const_dir)
	print("calculating number of constructions")
	for number_file in number_list:
		curr_file = number_const_dir + number_file
		print("reading from %s" % curr_file)
		df = pd.read_csv(curr_file)
		df = df.drop(labels=" ", axis=1)
		for key in df.keys():
			for i in range(NUM_TIME_STEPS/DIVISIONS):
				#add the number of constructions known by the learner at
				#that time step
				#if out of range, it means that all learners finished in
				#that file before that time step, so the learner is assumed
				#to know all of the constructions
				try:
					num_constructions[key.strip()][i] += df[key][i]
				except:
					num_constructions[key.strip()][i] += len(constructions)
	
	#take the averages of the number for each learner at each time step
	for learner in learners:
		for i in range(NUM_TIME_STEPS/DIVISIONS):
			num_constructions[learner][i] = float(num_constructions[learner][i]) / len(number_list)

	#write to file
	number_out = directory + "/number_constructions.csv"
	with open(number_out, "w+") as outfile:
		first_line = ""
		for learner in learners:
			first_line += learner + ", "
		outfile.write(first_line + "\n")
		for i in range(NUM_TIME_STEPS/DIVISIONS):
			curr_line = ""
			for learner in learners:
				curr_line += str(num_constructions[learner][i]) + ", "
			outfile.write(curr_line + "\n")
			

	##########################################
	#				Order
	##########################################
	order_list = os.listdir(order_dir)
	print(order_list)
	print("calculating Order")
	for order_file in order_list:
		curr_file = order_dir + order_file
		print("reading from %s" % curr_file)
		df = pd.read_csv(curr_file)
		df = df.drop(labels=" ", axis=1)
		for key in df.keys():
			#keep track of which constructions have been seen
			seen_const = []
			for i in range(len(constructions)):
				curr_const = df[key][i].strip()
				seen_const.append(curr_const)
				if (curr_const != "-"):
					order[key.strip()][curr_const] += i
			#for every construction not in seen_const, add len(constructions)
			for const in constructions:
				if (const not in seen_const):
					order[key.strip()][const] += len(constructions) 


	#average for constructions
	for learner in learners:
		for const in constructions:
			order[learner][const] = order[learner][const]/len(order_list)

	#sort order in order to output
	sorted_order = {}
	for learner in learners:
		sorted_order[learner] = sorted(order[learner].items(), key=operator.itemgetter(1))

	
	#write to file
	order_out = directory + "/order.csv"
	with open(order_out, "w+") as outfile:
		first_line = ""
		for learner in learners:
			first_line += learner + ", "
		outfile.write(first_line + "\n")
		for i in range(len(constructions)):
			curr_line = ""
			for learner in learners:
				curr_line += ("%s: %s, " % (sorted_order[learner][i][0], sorted_order[learner][i][1]))
			outfile.write(curr_line + "\n")


'''
Consolidate num results

	purpose: takes the num_construction files generated by get_results and uses them to 
				create charts to make the data understandable
	inputs: directory is the directory where the files are stored
			learners is the list of learner names (should be a list of
				strings)
			constructions is a list of constructions
			divisions is the number of times to divide the output into. For example,
				if there are 5000 time steps and divisions equals 10, then the
				output file will contain the progress at 500, 1000, etc...
	outputs: finds the order in which learners finished learning (or didn't),
				sorts based on this, and then produces a file that contains 
				the progress of every learner at varioius time steps (suitable
				for a bar graph)
'''
def consolidate_num_results(directory, constructions, divisions):
	filename = directory + "/number_constructions.csv"
	df = pd.read_csv(filename)
	df = df.drop(labels=" ", axis=1)
	#keep track of when learners finish {learner: steps to finish}
	finish_time = {}
	total_steps = NUM_TIME_STEPS/DIVISIONS
	finished_num = len(constructions) * (9.0/10) 

	for i in range(total_steps):
		for learner in df.keys():
			if (learner not in finish_time.keys()):
				#wait for learner to learn 9/10ths of constructions
				if (df[learner][i]  >= (finished_num)):
					finish_time[learner] = i * DIVISIONS

					
	#for every learner that didn't finish on average, add to finish_time
	for learner in df.keys():
		if (learner not in finish_time.keys()):
			#make the time one over what is possible
			finish_time[learner] = NUM_TIME_STEPS + 1

	#rank file lists the number of time steps to finish learning
	rank_file = directory + "/ranked_learners.csv"
	#progress file lists the number of known constructions at various time steps
	progress_file = directory + "/learner_progress.csv"

	with open(rank_file, "w+") as rankfile:
		for key, value in sorted(finish_time.iteritems(), key=lambda (k,v): (v,k)):
			rankfile.write("%s, %s\n" % (key, value))

	#get time steps to report
	report = []
	for i in range(1, divisions + 1):
		report.append(total_steps/divisions * i -1)
	with open(progress_file, "w+") as progressfile:
		first_line = ""
		for key, value in sorted(finish_time.iteritems(), key=lambda (k,v): (v, k)):
			first_line += "%s, " % key
		progressfile.write(first_line + "\n")
		for i in report:
			curr_line = ""
			for key, value in sorted(finish_time.iteritems(), key=lambda (k,v): (v, k)):
				curr_line += "%s, " % (df[key][i])
			progressfile.write(curr_line + "\n")


'''
Consolidate Order results

	purpose: uses the order data to create information about which constructions
				were learned first over all constructors and over specific typesm
	inputs: directory is the directory where the files are stored
			constructions is a list of constructions
	outputs: creates 4 files:
				1) overall order information- averages the orders across 
					all learner models
				2) frequentist- averages the orders across frequentist models
				3) ComplexityBased- averages the orders across ComplexityBased
					models
				4) threshold- averages the orders across threshold models
'''
def consolidate_order_results(directory, constructions):
	infile_name = directory + "/order.csv"
	outfile_name = directory + "/order_info.csv"

	#dicts to store order info
	overall_orders = {}
	frequentist_orders = {}
	complexityBased_orders = {}
	threshold_orders = {}

	#counters to store info on the numbers of each model
	overall_count = 0
	frequentist_count = 0
	complexityBased_count = 0
	threshold_count = 0

	df = pd.read_csv(infile_name)
	df = df.drop(labels=" ", axis=1)
	#initialize dicts
	for i in range(len(constructions)):
		curr_const = df[df.keys()[0]][i].split(":")[0].strip()
		overall_orders[curr_const] = 0
		frequentist_orders[curr_const] = 0
		complexityBased_orders[curr_const] = 0
		threshold_orders[curr_const] = 0

	for key in df.keys():
		#update counts
		overall_count += 1
		if ("frequentist" in key):
			frequentist_count += 1
		if ("ComplexityBased" in key):
			complexityBased_count += 1
		if ("Threshold" in key):
			threshold_count += 1


		for i in range(len(constructions)):
			curr_const = df[key][i].split(":")[0].strip()
			curr_value = int(df[key][i].split(":")[1])

			#update order info
			overall_orders[curr_const] += curr_value 
			if ("frequentist" in key):
				frequentist_orders[curr_const] += curr_value
			if ("ComplexityBased" in key):
				complexityBased_orders[curr_const] += curr_value
			if ("Threshold" in key):
				threshold_orders[curr_const] += curr_value

	#average order info
	for const in overall_orders:
		overall_orders[const] = overall_orders[const] / overall_count
		frequentist_orders[const] = frequentist_orders[const] / frequentist_count
		complexityBased_orders[const] = complexityBased_orders[const] / complexityBased_count
		threshold_orders[const] = threshold_orders[const] / threshold_count

	#sort dicts
	#overall
	sorted_overall = []
	for key, value in sorted(overall_orders.iteritems(), key=lambda (k,v): (v, k)):
		sorted_overall.append([key, value])
	#frequentist
	sorted_frequentist = []
	for key, value in sorted(frequentist_orders.iteritems(), key=lambda (k,v): (v, k)):
		sorted_frequentist.append([key, value])
	#complexityBased
	sorted_complexityBased = []
	for key, value in sorted(complexityBased_orders.iteritems(), key=lambda (k,v): (v, k)):
		sorted_complexityBased.append([key, value])
	#threshold
	sorted_threshold = []
	for key, value in sorted(threshold_orders.iteritems(), key=lambda (k,v): (v, k)):
		sorted_threshold.append([key, value])


	#write to file
	with open(outfile_name, "w+") as outfile:
		first_line = "Overall, Frequentist, ComplexityBased, Threshold\n"
		outfile.write(first_line)
		for i in range(len(sorted_overall)):
			curr_line = "%s:%s, %s:%s, %s:%s, %s:%s\n" % (sorted_overall[i][0], sorted_overall[i][1],
				sorted_frequentist[i][0], sorted_frequentist[i][1], sorted_complexityBased[i][0],
				sorted_complexityBased[i][1], sorted_threshold[i][0], sorted_threshold[i][1])
			outfile.write(curr_line)



'''
run theoretical experiment

	purpose: runs experiments based on a probabilistic generation from a 
				distribution to see the rate at which different learners
				acquire all cosntructions and the order they acquire them
				in.
	inputs: constructions is a list of constructions
			distribution is a list of probabilities associated with each
				construction
			learners is a dict of named learners
			output_dir is where the output should be stored
			times is the number of times to run through the experiment 
				before averaging to find results

	outputs:will write two sets of files:
				1) number of constructions at each time step, which
					will go in output_dir/number_constructions/
					example file:
						learner1, 			learner2, 			...
						l1 constr # at t1, l2 constr # at t1, 	...
						l1 constr # at t2, l2 constr # at t2, 	...
				2) order the constructions are acquired in
					will go in output_dir/order/
					example file:
						learner1,	learner2, 	...
						l1_const1,	l2_const1,	...
						l1_const2,	l2_const2,	...

'''
def run_theoretical_experiments(constructions, distribution, learners, output_dir, times):
	#print("Running theoretical experiments")
	construction_num = len(constructions)

	#make directories if necessary
	if not os.path.exists(output_dir):
		#print("Making directory '%s'" % output_dir)
		os.makedirs(output_dir)

	outfile1_dir = output_dir + "/number_constructions/"
	outfile2_dir = output_dir + "/order/"
	if not os.path.exists(outfile1_dir):
		#print("Making directory '%s'" % outfile1_dir)
		os.makedirs(outfile1_dir)
	if not os.path.exists(outfile2_dir):
		#print("Making directory '%s'" % outfile2_dir)
		os.makedirs(outfile2_dir)

	for iteration in range(times):
		#print("beginning iteration number %s" % iteration)
		outfile1_name = outfile1_dir + str(iteration)
		outfile2_name = outfile2_dir + str(iteration)
		finished_iteration = False

		#write learner names for experiments 1 and 2
		#experiment 1
		with open(outfile1_name, "w+") as outfile1:
			curr_line = ""
			for learner in learners.keys():
				curr_line += str(learner) 
				curr_line += ", "
			curr_line += "\n"
			outfile1.write(curr_line)
		#experiment 2
		with open(outfile2_name, "w+") as outfile2:
			curr_line = ""
			for learner in learners.keys():
				curr_line += str(learner) 
				curr_line += ", "
			curr_line += "\n"
			outfile2.write(curr_line)

		#wait until every learner knowns every construction
		input_num = 1
		while(not finished_iteration):
			#print("input number %s" % input_num)
			#feed random construction to learners
			curr_input = np.random.choice(constructions, p=distribution)
			for learner in learners.keys():
				learners[learner].take_input(curr_input)

			#write the current state of each learner for experiment 
			#only do this every 10th step
			if (input_num % 10 == 0):
				with open(outfile1_name, "a") as outfile1:
					curr_line = ""
					for learner in learners.keys():
						curr_line += str(len(learners[learner].get_known()))
						curr_line += ", "
					curr_line += "\n"
					outfile1.write(curr_line)


			#check if every learner knows every construction
			#total complete is the number of learners that know all of 
			#the vocab
			total_complete = 0
			for learner in learners.keys():
				if (len(learners[learner].get_known()) == construction_num):
					total_complete += 1
			if (total_complete == len(learners.keys())):
				finished_iteration = True

			input_num += 1
			print("step number " + str(input_num))
			if (input_num > NUM_TIME_STEPS):
				finished_iteration = True

		#write order for experiment 2
		with open(outfile2_name, "a") as outfile2:
			#write learner names
			curr_line = ""
			for learner in learners.keys():
				curr_line += str(learner)
				curr_line += ", "
			curr_line += "\n"

			#write order
			for order_number in range(construction_num):
				curr_line = ""
				for learner in learners.keys():
					#if the learner hasn't picked up that number of constructions,
					#write "-" instead
					try:
						curr_line += str(learners[learner].get_known()[order_number])
					except:
						curr_line += "-"
					curr_line += ", "
				curr_line += "\n"
				outfile2.write(curr_line)
		for learner in learners.keys():
			learners[learner].reset()



	'''
	#consolidate results
	get_results(output_dir, learners.keys())
	'''


def main():
	###############################################################
	#	get constructions and data_distribution from speech data
	###############################################################
	print("Extracting data")
	speech_data = Extract_data.SpeechData()
	speech_data.add_from_dir(DATA_DIR)
	data_distribution = speech_data.get_construction_likelihoods()
	#extract to two seperate lists for convenience
	constructions = []
	distribution_observed = []
	for key in data_distribution.keys():
		constructions.append(key)
		distribution_observed.append(data_distribution[key])

	###########################################
	#			Set up distributions
	###########################################
	print("Setting up distributions")
	distributions = {}
	distributions["observed"] = distribution_observed
	distributions["uniform"] = [(1.0/len(constructions)) for i in range(len(constructions))]

	######################################
	#			set up learners
	######################################
	print("Setting up Learners")
	learners = {}
	#frequentist
	learners["frequentist_1"] = Learner.FrequentistLearner(learn_times=1)
	learners["frequentist_2"] = Learner.FrequentistLearner(learn_times=2)
	learners["frequentist_3"] = Learner.FrequentistLearner(learn_times=3)
	learners["frequentist_5"] = Learner.FrequentistLearner(learn_times=5)
	learners["frequentist_10"] = Learner.FrequentistLearner(learn_times=10)
	learners["frequentist_15"] = Learner.FrequentistLearner(learn_times=15)
	#Complexity-Based
	learners["ComplexityBased_1"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:1})
	learners["ComplexityBased_09_1"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.9})
	learners["ComplexityBased_09_2"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.9, 2.0:0.8})
	learners["ComplexityBased_09_3"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.9, 2.0:0.7})
	learners["ComplexityBased_09_4"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.9, 2.0:0.6})
	learners["ComplexityBased_09_5"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.9, 2.0:0.5})
	learners["ComplexityBased_09_6"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.9, 2.0:0.4})
	learners["ComplexityBased_09_7"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.9, 2.0:0.4, 3.0:0.2})
	learners["ComplexityBased_09_8"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.9, 2.0:0.4, 2.5: 0.2, 3.0:0.1})
	learners["ComplexityBased_09_9"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.9, 2.0:0.4, 2.5: 0.1, 3.0:0.05})
	learners["ComplexityBased_08_1"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.8})
	learners["ComplexityBased_08_2"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.8, 2.0:0.7})
	learners["ComplexityBased_08_3"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.8, 2.0:0.6})
	learners["ComplexityBased_08_4"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.8, 2.0:0.5})
	learners["ComplexityBased_08_5"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.8, 2.0:0.4})
	learners["ComplexityBased_08_6"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.8, 2.0:0.4, 3.0:0.2})
	learners["ComplexityBased_08_7"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.8, 2.0:0.4, 2.5: 0.2, 3.0:0.1})
	learners["ComplexityBased_08_8"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.8, 2.0:0.4, 2.5: 0.1, 3.0:0.05})
	learners["ComplexityBased_07_1"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.7})
	learners["ComplexityBased_07_2"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.7, 2.0:0.6})
	learners["ComplexityBased_07_3"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.7, 2.0:0.5})
	learners["ComplexityBased_07_4"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.7, 2.0:0.4})
	learners["ComplexityBased_07_5"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.7, 2.0:0.4, 3.0:0.2})
	learners["ComplexityBased_07_6"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.7, 2.0:0.4, 2.5: 0.2, 3.0:0.1})
	learners["ComplexityBased_07_7"] = Learner.ComplexityBasedLearner(probability_dict={0.5: 1, 1.0:0.7, 2.0:0.4, 2.5: 0.1, 3.0:0.05})
	#Threshold
	learners["Threshold_10_10"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:10})
	learners["Threshold_10_8_1"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:8})
	learners["Threshold_10_8_2"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:8, 1.5:7, 2.0:6})
	learners["Threshold_10_8_"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:8, 1.5:7, 2.0:6, 3.0:4})
	learners["Threshold_10_8_3"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:8, 1.5:7, 2.0:6, 2.5: 4, 3.0: 2})
	learners["Threshold_10_8_4"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:8, 1.5:7, 2.0:6, 2.5: 4, 3.0: 2})
	learners["Threshold_10_8_5"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:8, 1.5:7, 2.0:6, 2.5: 4, 3.0: 1})
	learners["Threshold_10_8_6"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:8, 1.5:7, 2.0:6, 2.5: 4})
	learners["Threshold_10_8_7"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:8, 1.5:7, 2.0:6, 2.5: 3, 3.0: 2})
	learners["Threshold_10_8_8"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:8, 1.5:7, 2.0:6, 2.5: 3, 3.0: 1})
	learners["Threshold_10_8_9"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:8, 1.5:7, 2.0:6, 2.5: 3})
	learners["Threshold_10_8_10"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:8, 1.5:7, 2.0:6, 2.5: 1})
	learners["Threshold_10_6_1"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:6})
	learners["Threshold_10_6_2"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:6, 1.5: 5, 2.0: 4})
	learners["Threshold_10_6_3"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:6, 1.5: 4, 2.0: 2})
	learners["Threshold_10_6_4"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:6, 1.5:4, 2.0: 2, 2.5: 1})
	learners["Threshold_10_6_5"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:6, 1.5:4, 2.0: 1})
	learners["Threshold_10_6_6"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:6, 1.5:2, 2.0: 1})
	learners["Threshold_10_6_7"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:6, 1.5:3, 2.0: 2})
	learners["Threshold_10_6_8"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:6, 1.5:2, 2.0: 1})
	learners["Threshold_10_4_1"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:4, 1.5:3, 2.0: 2})
	learners["Threshold_10_4_2"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:4, 1.5:2, 2.0: 1})
	learners["Threshold_10_2"] = Learner.ThresholdLearner(complexity_dict={0.5: 10, 1.0:2, 1.5:1})
	learners["Threshold_8_6_1"] = Learner.ThresholdLearner(complexity_dict={0.5: 8, 1.0:6})
	learners["Threshold_8_6_2"] = Learner.ThresholdLearner(complexity_dict={0.5: 8, 1.0:6, 1.5: 5, 2.0: 4})
	learners["Threshold_8_6_3"] = Learner.ThresholdLearner(complexity_dict={0.5: 8, 1.0:6, 1.5: 4, 2.0: 2})
	learners["Threshold_8_6_4"] = Learner.ThresholdLearner(complexity_dict={0.5: 8, 1.0:6, 1.5:4, 2.0: 2, 2.5: 1})
	learners["Threshold_8_6_5"] = Learner.ThresholdLearner(complexity_dict={0.5: 8, 1.0:6, 1.5:4, 2.0: 1})
	learners["Threshold_8_6_6"] = Learner.ThresholdLearner(complexity_dict={0.5: 8, 1.0:6, 1.5:2, 2.0: 1})
	learners["Threshold_8_6_7"] = Learner.ThresholdLearner(complexity_dict={0.5: 8, 1.0:6, 1.5:3, 2.0: 2})
	learners["Threshold_8_6_8"] = Learner.ThresholdLearner(complexity_dict={0.5: 8, 1.0:6, 1.5:2, 2.0: 1})
	learners["Threshold_8_4_1"] = Learner.ThresholdLearner(complexity_dict={0.5: 8, 1.0:4, 1.5:3, 2.0: 2})
	learners["Threshold_8_4_2"] = Learner.ThresholdLearner(complexity_dict={0.5: 8, 1.0:4, 1.5:2, 2.0: 1})
	learners["Threshold_8_2"] = Learner.ThresholdLearner(complexity_dict={0.5: 8, 1.0:2, 1.5:1})
	learners["Threshold_6_4_1"] = Learner.ThresholdLearner(complexity_dict={0.5: 6, 1.0:4, 1.5:3, 2.0: 2})
	learners["Threshold_6_4_2"] = Learner.ThresholdLearner(complexity_dict={0.5: 6, 1.0:4, 1.5:2, 2.0: 1})
	learners["Threshold_6_2"] = Learner.ThresholdLearner(complexity_dict={0.5: 6, 1.0:2, 1.5:1})


	###################################################
	#		Run Experiments on Artificial Data
	###################################################
	#uniform
	run_theoretical_experiments(constructions, distributions["uniform"], learners, UNIFORM_OUT_DIRECTORY, TIMES_TO_RUN)
	learners_as_string = []
	for learner in learners.keys():
		learners_as_string.append(learner.strip())
	get_results(UNIFORM_OUT_DIRECTORY, learners_as_string, constructions)
	
	#consolidate_num_results(UNIFORM_OUT_DIRECTORY, constructions, DIVISIONS)
	#consolidate_order_results(UNIFORM_OUT_DIRECTORY, constructions)

	#observed
	run_theoretical_experiments(constructions, distributions["observed"], learners, OBSERVED_OUT_DIRECTORY, TIMES_TO_RUN)
	learners_as_string = []
	for learner in learners.keys():
		learners_as_string.append(learner.strip())
	get_results(OBSERVED_OUT_DIRECTORY, learners_as_string, constructions)
	
	#consolidate_num_results(OBSERVED_OUT_DIRECTORY, constructions, DIVISIONS)
	#consolidate_order_results(OBSERVED_OUT_DIRECTORY, constructions)




if __name__ == "__main__":
	DATA_DIR = sys.argv[1]
	if (DATA_DIR[-1] != "/"):
		DATA_DIR += "/"
	UNIFORM_OUT_DIRECTORY = "results/artificial_data/" + DATA_DIR + "uniform/"
	OBSERVED_OUT_DIRECTORY = "results/artificial_data/" + DATA_DIR + "observed/"
	main()
